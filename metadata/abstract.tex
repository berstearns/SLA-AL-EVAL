\abstract{
    Large Language Models (LLMs) have shown remarkable capabilities in simulating human behavior, yet their application to modeling second language acquisition patterns remains underexplored.
    This paper investigates whether generative LLMs can authentically simulate second language learners' responses across two fundamental task types: open-ended production (aligned with Next Word Prediction, NWP) and fill-in-the-gap exercises (aligned with Masked Language Modeling, MLM).
    We evaluate specialized LLMs—models fine-tuned for NWP versus MLM objectives—on their ability to generate learner-like responses at different CEFR proficiency levels (A1-C2).
    Our key research question examines whether task-specific model architectures lead to better simulation fidelity: do NWP-specialized models excel at production tasks while MLM-specialized models perform better on fill-in-the-gap exercises?
    We develop a prompt engineering framework adapted from student simulation research, enabling LLMs to role-play as learners of varying proficiency.
    Evaluation is conducted on three learner corpora spanning multiple L1 backgrounds and proficiency levels, comparing simulated responses against authentic learner productions using metrics including response accuracy patterns, error type distributions, and lexical/syntactic complexity measures.
    Our findings reveal that (1) generative LLMs can simulate proficiency-specific response patterns in both task types, though with varying degrees of authenticity, (2) task-model alignment (NWP for production, MLM for gap-filling) shows measurable but modest improvements, and (3) cross-task generalization differs significantly across model architectures.
    This work contributes to understanding how LLM training objectives shape their capacity for learner simulation, with implications for virtual pretesting, adaptive learning systems, and synthetic learner data generation for low-resource proficiency levels.
    \\ \newline \Keywords{artificial learners, LLM simulation, second language acquisition, generative models, CEFR, student modeling}
}
